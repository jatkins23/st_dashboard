import os
os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "TRUE")   # tolerate dual OpenMP on macOS
os.environ.setdefault("OMP_NUM_THREADS", "1")           # keep FAISS/OpenMP single-threaded
os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")    # tame Apple Accelerate
os.environ.setdefault("MKL_NUM_THREADS", "1")           # harmless on macOS

import sys
import hashlib
import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union

import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
from PIL import Image

from .clip_embeddings import CLIPEmbedder
from .vector_db import ImageEmbedding, VectorDB, create_database_if_not_exists
from .utils import extract_year_from_path
from .utils_loc import location_key, location_hash
from .whitening import compute_whitener

from multiprocessing import get_context

#this is a messy code that I want to refactor later -
# Planning to create a abstract Embedder class and have CLIPEmbedder inherit from it
# perhaps i need to have AI add comments and docstrings as well, Pycharm's autogenerated ones are not great

def _faiss_available() -> bool:
    try:
        import faiss  # type: ignore  # noqa: F401
        return True
    except ModuleNotFoundError:
        return False

EmbedderType = CLIPEmbedder


def _make_embedder(
    encoder: str,
    *,
    device: str | None,
    batch_size: int,
    clip_model: str,
    clip_pretrained: str,
    eva_model: str,
    eva_pretrained: str,
) -> EmbedderType:
    name = encoder.strip().lower()
    if name in {"clip", "clip_vitl14"}:
        model = clip_model
        pretrained_tag = clip_pretrained
        if name == "clip_vitl14":
            model = "ViT-L-14"
            pretrained_tag = "laion2b_s32b_b82k"
        return CLIPEmbedder(
            model_name=model,
            pretrained=pretrained_tag,
            device=device or os.getenv("DEVICE"),
            batch_size=batch_size,
        )
    if name == "eva_clip":
        # EVA-CLIP models are served via open_clip
        return CLIPEmbedder(
            model_name=eva_model,
            pretrained=eva_pretrained,
            device=device or os.getenv("DEVICE"),
            batch_size=batch_size,
        )
    raise ValueError(f"Unsupported encoder '{encoder}'. Expected one of: clip, eva_clip.")


def _faiss_build_child(
    npy_path: str,
        out_index_path: str,
        dim: int,
        use_hnsw: bool,
        m: int,
        ef_construction: int
) -> None:
    """
    Child-process entrypoint for building a FAISS index from a saved (N,D) float32 array.
    Ensures a WRITEABLE, CONTIGUOUS matrix; row-normalizes so inner product == cosine.
    """
    import os
    os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "TRUE")
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")

    import numpy as np
    import faiss

    try:
        faiss.omp_set_num_threads(1)
    except Exception:
        pass

    X_ro = np.load(npy_path, mmap_mode="r")
    if X_ro.ndim != 2 or X_ro.shape[1] != dim:
        raise ValueError(f"Expected (N,{dim}) in {npy_path}, got {X_ro.shape}")
    X = np.array(X_ro, dtype=np.float32, copy=True, order="C")  # WRITEABLE

    # L2-normalize rows so IP == cosine
    norms = np.linalg.norm(X, axis=1, keepdims=True)
    np.divide(X, np.clip(norms, 1e-12, None), out=X)

    if use_hnsw:
        index = faiss.IndexHNSWFlat(dim, m, faiss.METRIC_INNER_PRODUCT)
        index.hnsw.efConstruction = int(max(16, ef_construction))
    else:
        index = faiss.IndexFlatIP(dim)

    index.add(X)
    faiss.write_index(index, str(out_index_path))

def _spawn_build(npy_path: Path, out_index_path: Path, dim: int,
                 use_hnsw: bool, m: int, ef_construction: int) -> bool:
    ctx = get_context("spawn")
    p = ctx.Process(
        target=_faiss_build_child,
        args=(str(npy_path), str(out_index_path), int(dim),
              bool(use_hnsw), int(m), int(ef_construction)),
        daemon=True,
    )
    p.start()
    p.join()
    return (p.exitcode == 0) and out_index_path.exists() and out_index_path.stat().st_size > 0

def build_faiss_from_array(
    X: np.ndarray,
    out_dir: Path,
    base_name: str,
    dim: int,
    use_hnsw: bool = True,
    m: int = 16,
    ef_construction: int = 64,
) -> Path:
    """
    Safe, portable FAISS builder for any (N,D) float32 array. Writes:
      - {base_name}_{hnsw|flatip}.faiss
    Returns the index path.
    """
    if not _faiss_available():
        out_dir.mkdir(parents=True, exist_ok=True)
        note = out_dir / f"{base_name}_faiss_missing.txt"
        note.write_text(
            "FAISS not installed; index build skipped. Install with `uv pip install \"faiss-cpu>=1.7.4\"`.\n",
            encoding="utf-8",
        )
        print(f"[warn] FAISS not installed; skipping index build for '{base_name}'. Install with `uv pip install \"faiss-cpu>=1.7.4\"`.")
        return note
    out_dir.mkdir(parents=True, exist_ok=True)
    x_npy = out_dir / f"{base_name}_data.npy"
    np.save(x_npy, X.astype("float32", copy=False))

    force_flat = os.environ.get("FAISS_FORCE_FLAT", "").strip() == "1"
    if not force_flat and use_hnsw:
        idx_path = out_dir / f"{base_name}_hnsw.faiss"
        ok = _spawn_build(x_npy, idx_path, dim, True, m, ef_construction)
        if ok:
            print(f"Built FAISS HNSW index: {idx_path}")
            return idx_path
        print("HNSW build failed. Falling back to exact FlatIP…")

    idx_path = out_dir / f"{base_name}_flatip.faiss"
    ok = _spawn_build(x_npy, idx_path, dim, False, m, ef_construction)
    if not ok:
        raise RuntimeError("FAISS index build failed even with FlatIP fallback.")
    print(f"Built FAISS FlatIP index: {idx_path}")
    return idx_path


def build_change_vectors(
    loc_keys: Sequence[str],
    years: Sequence[int],
    E: np.ndarray,
    only_consecutive: bool = True,
    valid_mask: Optional[Sequence[bool]] = None,
) -> Tuple[np.ndarray, List[str]]:
    """
    Compute Δ embeddings per location across years.
    loc_keys should be stable per physical location (e.g. path with year stripped).
    Δ = z_{y2} - z_{y1}; each Δ L2-normalized for cosine usage.

    Parameters
    ----------
    only_consecutive : bool
        If True, only adjacent year pairs are considered. If False, every
        strictly increasing year combination contributes a Δ vector.
    """
    loc2rows: Dict[str, List[int]] = {}
    for i, (key, y) in enumerate(zip(loc_keys, years)):
        loc2rows.setdefault(str(key), []).append(i)

    deltas: List[np.ndarray] = []
    delta_ids: List[str] = []

    for key, idxs in loc2rows.items():
        if valid_mask is not None:
            idxs = [i for i in idxs if bool(valid_mask[i])]
            if len(idxs) < 2:
                continue
        ys = [(years[i], i) for i in idxs]
        ys.sort(key=lambda t: t[0])
        if len(ys) < 2:
            continue
        pairs = list(zip(ys[:-1], ys[1:])) if only_consecutive else [
            (ys[a], ys[b]) for a in range(len(ys)) for b in range(a + 1, len(ys))
        ]
        for (y1, i1), (y2, i2) in pairs:
            d = E[i2] - E[i1]
            n = np.linalg.norm(d) + 1e-12
            d = (d / n).astype("float32", copy=False)
            deltas.append(d)
            delta_ids.append(f"{key}::{y1}->{y2}")

    D = np.vstack(deltas).astype("float32", copy=False) if deltas else np.zeros((0, E.shape[1]), dtype="float32")
    return D, delta_ids


MASK_EXTENSIONS = (".png", ".jpg", ".jpeg", ".tif", ".tiff")


@dataclass(frozen=True)
class MaskClassDef:
    label: str
    value: Optional[int] = None
    color: Optional[Tuple[int, int, int]] = None

# Default class colours so users don't have to pass hex values every time.
DEFAULT_MASK_COLORS: Dict[str, Tuple[int, int, int]] = {
    "roadway": (0, 128, 0),
    "road": (0, 128, 0),
    "sidewalk": (0, 0, 255),
    "crosswalk": (255, 0, 0),
    "bike_lane": (171, 71, 188),
}

def _parse_color(value: str) -> Tuple[int, int, int]:
    token = value.strip()
    if token.startswith("#"):
        token = token.lstrip("#")
        if len(token) not in {6, 8}:
            raise ValueError(f"Invalid hex colour '{value}'")
        if len(token) == 8:
            token = token[2:]  # ignore alpha if provided
        r = int(token[0:2], 16)
        g = int(token[2:4], 16)
        b = int(token[4:6], 16)
        return (r, g, b)
    parts = token.split(",")
    if len(parts) != 3:
        raise ValueError(f"Colour '{value}' must be '#RRGGBB' or 'R,G,B'")
    r, g, b = (int(p) for p in parts)
    return (r, g, b)

def _to_fixed_size_list(mat: np.ndarray, dim: int, valid: Optional[np.ndarray] = None) -> pa.FixedSizeListArray:
    """
    Convert (N,dim) float32 to a FixedSizeListArray.
    To avoid Parquet list/null limitations, invalid rows (when provided) are zero-filled instead of null.
    """
    arr = mat.astype("float32", copy=False)
    if valid is not None:
        mask = np.asarray(valid, dtype=bool)
        if mask.shape[0] != arr.shape[0]:
            raise ValueError("valid mask length does not match matrix rows")
        # zero-fill invalid rows to avoid null lists
        if (~mask).any():
            arr = arr.copy()
            arr[~mask] = 0.0
    flat = arr.reshape(-1)
    base = pa.array(flat, type=pa.float32())
    return pa.FixedSizeListArray.from_arrays(base, dim)

def _write_parquet_shards(
    out_dir: Path,
    shard_size: int,
    paths: List[str],
    years: List[int],
    loc_keys: List[str],
    embeddings: np.ndarray,
    mask_embeddings: np.ndarray,
    mask_valid: np.ndarray,
    fusion_embeddings: np.ndarray,
    mask_image_embeddings: np.ndarray,
    mask_image_valid: np.ndarray,
    mask_paths: np.ndarray,
    mask_stats: np.ndarray,
    checkpoint_dir: Optional[Path],
    resume: bool = False,
) -> None:
    shard_root = out_dir / "shards"
    shard_root.mkdir(parents=True, exist_ok=True)
    cp_dir = checkpoint_dir or (out_dir / "checkpoints")
    cp_dir.mkdir(parents=True, exist_ok=True)
    manifest_path = shard_root / "manifest.parquet"
    if resume and manifest_path.exists():
        print(f"[shards] Resume enabled and manifest found; skipping shard rewrite: {manifest_path}")
        return

    def _digest(cols: Dict[str, pa.Array]) -> str:
        h = hashlib.blake2b(digest_size=16)
        for key in sorted(cols.keys()):
            buf = cols[key].to_pylist()
            h.update(str(key).encode("utf-8"))
            h.update(str(buf).encode("utf-8"))
        return h.hexdigest()
    manifest_rows = []
    n = len(paths)
    dim = embeddings.shape[1]
    shard_idx = 0
    for start in range(0, n, shard_size):
        end = min(start + shard_size, n)
        sl = slice(start, end)
        shard_path = shard_root / f"shard_{shard_idx:05d}.parquet"
        tmp_path = shard_path.with_suffix(".tmp")
        # Drop rows with invalid optional features so Parquet lists stay clean.
        keep = np.ones(end - start, dtype=bool)
        if mask_embeddings.size:
            keep &= mask_valid[sl]
        if mask_image_embeddings.size:
            keep &= mask_image_valid[sl]
        dropped = (~keep).sum()
        if keep.sum() == 0:
            shard_idx += 1
            continue
        keep_idx = np.nonzero(keep)[0]
        loc_ids = [location_hash(loc_keys[sl][i]) for i in keep_idx]
        cols = {
            "path": pa.array([paths[sl][i] for i in keep_idx], type=pa.string()),
            "year": pa.array([years[sl][i] for i in keep_idx], type=pa.int32()),
            "location_key": pa.array([loc_keys[sl][i] for i in keep_idx], type=pa.string()),
            "location_id": pa.array(loc_ids, type=pa.int64()),
            "embedding": _to_fixed_size_list(embeddings[sl][keep_idx], dim),
        }
        if mask_embeddings.size:
            cols["mask_embedding"] = _to_fixed_size_list(mask_embeddings[sl][keep_idx], dim)
        if fusion_embeddings.size:
            cols["fusion_embedding"] = _to_fixed_size_list(fusion_embeddings[sl][keep_idx], dim)
        if mask_image_embeddings.size:
            cols["mask_image_embedding"] = _to_fixed_size_list(mask_image_embeddings[sl][keep_idx], dim)
        if mask_paths.size:
            cols["mask_path"] = pa.array([mask_paths[sl][i] for i in keep_idx], type=pa.string())
        if mask_stats.size:
            cols["mask_stats"] = pa.array(mask_stats[sl][keep_idx].tolist(), type=pa.string())
        tbl = pa.table(cols)
        pq.write_table(tbl, tmp_path, compression="zstd")
        digest = _digest(cols)
        tmp_path.replace(shard_path)
        manifest_rows.append(
            {
                "shard": shard_path.name,
                "rows": keep.sum(),
                "start": start,
                "end": end,
                "year_min": int(min(years[sl])),
                "year_max": int(max(years[sl])),
                "loc_id_min": int(min(loc_ids)),
                "loc_id_max": int(max(loc_ids)),
                "checksum": digest,
                "dropped_rows": int(dropped),
            }
        )
        with open(cp_dir / "last_shard.txt", "w", encoding="utf-8") as f:
            f.write(f"{shard_idx}")
        if dropped:
            drop_log = cp_dir / "dropped_shards.log"
            with open(drop_log, "a", encoding="utf-8") as f:
                f.write(f"{shard_path.name}: dropped {dropped} rows (invalid optional features)\n")
        shard_idx += 1
    if manifest_rows:
        keys = manifest_rows[0].keys()
        manifest_dict = {k: [row[k] for row in manifest_rows] for k in keys}
        manifest_tbl = pa.table(manifest_dict)
        pq.write_table(manifest_tbl, manifest_path)
    print(f"Wrote {len(manifest_rows)} shard(s) to {shard_root} (manifest: {manifest_path})")


def _build_faiss_for_shards(shard_root: Path, spaces: Sequence[str]) -> None:
    manifest_path = shard_root / "manifest.parquet"
    if not manifest_path.exists():
        print(f"[shards] No manifest found at {manifest_path}; skipping shard FAISS.")
        return
    manifest = pq.read_table(manifest_path).to_pydict()
    shard_names: List[str] = manifest.get("shard", [])
    if not shard_names:
        print("[shards] Empty manifest; skipping shard FAISS.")
        return

    for shard_name in shard_names:
        shard_path = shard_root / shard_name
        if not shard_path.exists():
            print(f"[warn] Shard missing, skipping FAISS build: {shard_path}")
            continue
        for space in spaces:
            col = space
            try:
                tbl = pq.read_table(shard_path, columns=[col])
            except Exception:
                continue
            arr = tbl.column(0)
            if not isinstance(arr.type, pa.FixedSizeListType):
                continue
            dim = arr.type.list_size
            # Convert to numpy, drop nulls
            np_arr = np.array(arr.to_pylist(), dtype=object)
            mask = np.array([x is not None for x in np_arr], dtype=bool)
            if not mask.any():
                continue
            X = np.vstack(np_arr[mask]).astype("float32", copy=False)
            base_name = f"{shard_path.stem}_{space}"
            try:
                build_faiss_from_array(
                    X=X,
                    out_dir=shard_root,
                    base_name=base_name,
                    dim=dim,
                    use_hnsw=True,
                    m=16,
                    ef_construction=64,
                )
            except Exception as exc:
                print(f"[warn] Failed to build FAISS for {shard_name} ({space}): {exc}")


def parse_mask_classes(raw: Optional[Sequence[str]]) -> List[MaskClassDef]:
    classes: List[MaskClassDef] = []
    if raw:
        for item in raw:
            if not item:
                continue
            for token in item.split(","):
                token = token.strip()
                if not token:
                    continue
                # Allow label-only entries and use defaults when known.
                if ":" not in token:
                    lbl = token
                    colour = DEFAULT_MASK_COLORS.get(lbl.lower())
                    classes.append(MaskClassDef(label=lbl, color=colour))
                    continue
                value_str, label = token.split(":", 1)
                label = label.strip() or f"class_{value_str}"
                if "," in value_str or value_str.strip().startswith("#"):
                    color = _parse_color(value_str)
                    classes.append(MaskClassDef(label=label, color=color))
                else:
                    classes.append(MaskClassDef(label=label, value=int(value_str)))
    if not classes:
        classes = [
            MaskClassDef(label="roadway", color=(0, 128, 0)),  # green
            MaskClassDef(label="sidewalk", color=(0, 0, 255)),  # blue
            MaskClassDef(label="crosswalk", color=(255, 0, 0)),  # red
        ]
    return classes


def find_mask_path(rel_path: str, mask_root: Path) -> Optional[Path]:
    rel = Path(rel_path)
    candidates = [mask_root / rel]
    stem = rel.with_suffix("")
    for ext in MASK_EXTENSIONS:
        candidates.append(mask_root / stem.with_suffix(ext))
    for candidate in candidates:
        if candidate.exists():
            return candidate
    return None


def compute_mask_features(
    image_path: Path,
    mask_path: Path,
    embedder: EmbedderType,
    class_defs: Sequence[MaskClassDef],
    base_embedding: np.ndarray,
    focus_weight: float,
    color_tolerance: int,
) -> Tuple[Optional[np.ndarray], np.ndarray, Dict[str, float]]:
    image = Image.open(image_path).convert("RGB")
    mask_img = Image.open(mask_path)
    mask_rgb = mask_img.convert("RGB")
    mask_array_rgb = np.array(mask_rgb, dtype=np.uint8)
    mask_array_gray = mask_img.convert("L") if mask_img.mode != "L" else mask_img
    mask_array_gray = np.array(mask_array_gray, dtype=np.uint8)

    rgb_array = np.array(image, dtype=np.uint8)
    total_pixels = float(mask_array_rgb.shape[0] * mask_array_rgb.shape[1])

    class_images: List[Image.Image] = []
    weights: List[float] = []
    stats: Dict[str, float] = {}

    tol = max(0, min(255, int(color_tolerance)))
    for class_def in class_defs:
        if class_def.color is not None:
            diff = np.abs(mask_array_rgb.astype(np.int16) - np.array(class_def.color, dtype=np.int16))
            region = np.all(diff <= tol, axis=-1)
        else:
            region = mask_array_gray == int(class_def.value)
        frac = float(region.sum()) / total_pixels
        stats[class_def.label] = frac
        if frac <= 0.0:
            continue
        masked_rgb = rgb_array.copy()
        masked_rgb[~region] = 0
        class_images.append(Image.fromarray(masked_rgb, mode="RGB"))
        weights.append(frac)

    recognised = sum(stats.values())
    stats["background"] = max(0.0, 1.0 - recognised)

    mask_embedding: Optional[np.ndarray] = None
    if class_images:
        embeddings = embedder.embed_pil_images(class_images)
        weights_arr = np.asarray(weights, dtype=np.float32)
        if embeddings.shape[0] == weights_arr.shape[0] and weights_arr.sum() > 0:
            mask_embedding = np.average(embeddings, axis=0, weights=weights_arr)
            norm = float(np.linalg.norm(mask_embedding))
            if norm > 1e-12:
                mask_embedding = (mask_embedding / norm).astype("float32", copy=False)
            else:
                mask_embedding = None

    fusion_embedding = base_embedding.astype("float32", copy=False)
    if mask_embedding is not None:
        roi_cover = min(1.0, recognised)
        weight = min(max(roi_cover, 0.2), max(0.0, min(1.0, focus_weight)))
        combined = ((1.0 - weight) * fusion_embedding) + (weight * mask_embedding)
        norm = float(np.linalg.norm(combined))
        if norm > 1e-12:
            fusion_embedding = (combined / norm).astype("float32", copy=False)

    image.close()
    mask_img.close()

    return mask_embedding, fusion_embedding, stats


def run_pipeline(
    image_folder: str,
    db_cfg: Dict[str, object],
    vector_dim: Optional[int] = None,
    ivf_lists: int = 100,
    device: str | None = None,
    batch_size: int = 64,
    encoder: str = "clip",
    clip_model: str = "ViT-B-32",
    clip_pretrained: str = "laion2b_s34b_b79k",
    eva_model: str = "EVA01-g-14",
    eva_pretrained: str = "laion2b_s4b_b79k",
    strict_year: bool = True,
    default_year: int | None = None,
    out_dir: str | Path = "artifacts",
    *,
    force_reindex: bool = False,
    compute_whitening: bool = True,
    whiten_max_samples: Optional[int] = 200000,
    delta_consecutive_only: bool = False,
    mask_dir: Optional[str] = None,
    mask_class_defs: Optional[List[MaskClassDef]] = None,
    mask_focus_weight: float = 0.5,
    mask_color_tolerance: int = 8,
    reuse_embeddings: bool = False,
    reset_table_on_mismatch: bool = False,
    shard_size: Optional[int] = None,
    shard_format: str = "parquet",
    checkpoint_dir: Optional[Path] = None,
    resume_shards: bool = False,
) -> None:
    root = Path(image_folder)
    if not root.exists():
        print(f"Error: folder not found: {root}")
        sys.exit(1)
    out = Path(out_dir); out.mkdir(parents=True, exist_ok=True)
    encoder_name = encoder.strip().lower()
    if encoder_name not in {"clip", "clip_vitl14", "eva_clip"}:
        raise ValueError(f"Unsupported encoder '{encoder}'; choose from: clip, clip_vitl14, eva_clip.")

    print(f"[encoder] Using {encoder_name.upper()} for embeddings.")

    print("=" * 70)
    print("IMAGE RETRIEVAL PIPELINE")
    print("=" * 70)

    focus_weight = max(0.0, min(1.0, float(mask_focus_weight)))
    class_defs: List[MaskClassDef] = list(mask_class_defs) if mask_class_defs else []
    mask_root: Optional[Path] = None
    if mask_dir:
        mask_root = Path(mask_dir).expanduser()
        if not mask_root.exists():
            raise FileNotFoundError(f"Mask folder not found: {mask_root}")
        if not class_defs:
            class_defs = parse_mask_classes(None)
        print(f"Using segmentation masks from {mask_root} with classes: {', '.join(cls.label for cls in class_defs)}")
    else:
        focus_weight = 0.0

    embedder: Optional[EmbedderType] = None
    cached_paths: Optional[List[str]] = None
    cached_embeddings: Optional[np.ndarray] = None
    resolved_vector_dim: Optional[int] = vector_dim
    if reuse_embeddings:
        paths_file = out / "state_paths.npy"
        embeddings_file = out / "state_embeddings.npy"
        if not paths_file.exists() or not embeddings_file.exists():
            raise FileNotFoundError(
                "Cannot reuse embeddings: state_paths.npy or state_embeddings.npy missing under artifacts."
            )
        cached_paths = np.load(paths_file, allow_pickle=True).tolist()
        cached_embeddings = np.load(embeddings_file)
        resolved_vector_dim = resolved_vector_dim or int(cached_embeddings.shape[1])
    else:
        embedder = _make_embedder(
            encoder_name,
            device=device,
            batch_size=batch_size,
            clip_model=clip_model,
            clip_pretrained=clip_pretrained,
            eva_model=eva_model,
            eva_pretrained=eva_pretrained,
        )
        if resolved_vector_dim is None:
            if encoder_name == "clip_vitl14":
                resolved_vector_dim = 768
            else:
                resolved_vector_dim = int(getattr(embedder, "embedding_dim", 0)) or 512
    assert resolved_vector_dim is not None

    # DB
    print("\nStep 1/4: Creating database...")
    print("-" * 70)
    create_database_if_not_exists(
        db_cfg["dbname"], 
        db_cfg["user"], 
        db_cfg["password"], 
        db_cfg["host"], db_cfg["port"]
    )
    print("Database created or already exists.")

    # schema
    print("\nStep 2/4: Setting up database schema...")
    print("-" * 70)
    with VectorDB(vector_dimension=resolved_vector_dim, **db_cfg) as db:
        existing_dim = db.get_column_dimension()
        if existing_dim is not None and existing_dim != resolved_vector_dim:
            if reset_table_on_mismatch:
                print(
                    f"Vector dimension changed (db={existing_dim}, expected={resolved_vector_dim}); "
                    "dropping embedding tables so they can be recreated."
                )
                db.reset_embeddings_tables()
            else:
                print(
                    f"Error: DB column dimension mismatch for image_embeddings.embedding "
                    f"(db={existing_dim}, expected={resolved_vector_dim}). "
                    "Use --reset-table-on-dim-mismatch to drop/recreate the table or pass a new --db-name."
                )
                sys.exit(1)
        db.setup_schema(ivf_lists=ivf_lists)
    print("Schema is ready.")

    # embeddings
    print(f"\nStep 3/4: Extracting {encoder_name.upper()} embeddings ...")
    print("-" * 70)
    pairs: List[Tuple[str, np.ndarray]]
    # embedder may already be initialised when computing fresh embeddings; otherwise created lazily

    existing_mask_lookup: Dict[str, Dict[str, object]] = {}
    existing_fusion_lookup: Dict[str, np.ndarray] = {}

    new_mask_vectors = 0
    reused_mask_vectors = 0
    new_mask_image_vectors = 0
    reused_mask_image_vectors = 0

    if reuse_embeddings:
        assert cached_paths is not None and cached_embeddings is not None
        pairs = [
            (str(cached_paths[i]), cached_embeddings[i].astype("float32", copy=False))
            for i in range(len(cached_paths))
        ]
        print(f"Reusing {len(pairs)} cached embeddings from {out}.")

        fusion_existing_path = out / "fusion_embeddings.npy"
        if fusion_existing_path.exists():
            fusion_existing = np.load(fusion_existing_path)
            for idx, path in enumerate(cached_paths):
                existing_fusion_lookup[str(path)] = fusion_existing[idx].astype("float32", copy=False)

        mask_emb_path = out / "mask_embeddings.npy"
        mask_valid_path = out / "mask_valid.npy"
        mask_paths_path = out / "mask_paths.npy"
        mask_stats_path = out / "mask_stats.npy"
        mask_image_emb_path = out / "mask_image_embeddings.npy"
        mask_image_valid_path = out / "mask_image_valid.npy"

        mask_emb_array = np.load(mask_emb_path) if mask_emb_path.exists() else None
        mask_valid_array = np.load(mask_valid_path) if mask_valid_path.exists() else None
        mask_paths_array_existing = np.load(mask_paths_path, allow_pickle=True) if mask_paths_path.exists() else None
        mask_stats_array_existing = np.load(mask_stats_path, allow_pickle=True) if mask_stats_path.exists() else None
        mask_image_array = np.load(mask_image_emb_path) if mask_image_emb_path.exists() else None
        mask_image_valid_array = np.load(mask_image_valid_path) if mask_image_valid_path.exists() else None

        for idx, path in enumerate(cached_paths):
            record: Dict[str, object] = {}
            if mask_emb_array is not None and mask_valid_array is not None and bool(mask_valid_array[idx]):
                record["mask_embedding"] = mask_emb_array[idx].astype("float32", copy=False)
            if mask_paths_array_existing is not None:
                mask_path_val = mask_paths_array_existing[idx]
                record["mask_path"] = str(mask_path_val) if mask_path_val is not None else None
            if mask_stats_array_existing is not None:
                stats_val = mask_stats_array_existing[idx]
                if stats_val is not None:
                    try:
                        match stats_val:
                            case str():
                                record["mask_stats"] = json.loads(stats_val)
                            case dict():
                                record["mask_stats"] = stats_val
                            case _:
                                record["mask_stats"] = None
                    except json.JSONDecodeError:
                        record["mask_stats"] = None
                else:
                    record["mask_stats"] = None
            if mask_image_array is not None and mask_image_valid_array is not None and bool(mask_image_valid_array[idx]):
                record["mask_image_embedding"] = mask_image_array[idx].astype("float32", copy=False)
            if record:
                existing_mask_lookup[str(path)] = record
    else:
        assert embedder is not None
        pairs = embedder.embed_folder(image_folder)
        if not pairs:
            print("No images found; aborting.")
            sys.exit(1)
        print(f"Extracted {len(pairs)} embeddings from {image_folder}.")

    root_resolved = root.resolve()

    def to_rel(path: str) -> str:
        p_obj = Path(path)
        try:
            rel = p_obj.resolve().relative_to(root_resolved)
            return str(rel)
        except Exception:
            return p_obj.name

    paths = [str(p) for p, _ in pairs]
    if mask_root is not None and embedder is None:
        embedder = _make_embedder(
            encoder_name,
            device=device,
            batch_size=batch_size,
            clip_model=clip_model,
            clip_pretrained=clip_pretrained,
            eva_model=eva_model,
            eva_pretrained=eva_pretrained,
        )

    years: List[int] = []
    rel_paths: List[str] = []
    loc_slugs: List[str] = []
    loc_hashes: List[int] = []
    rows_db: List[ImageEmbedding] = []
    mask_embeddings_raw: List[Optional[np.ndarray]] = []
    fusion_embeddings_list: List[np.ndarray] = []
    mask_paths: List[Optional[str]] = []
    mask_stats_list: List[Optional[Dict[str, float]]] = []
    mask_image_embeddings_list: List[Optional[np.ndarray]] = []

    total_pairs = len(pairs)

    for idx_loop, (p, e) in enumerate(pairs, start=1):
        rel = to_rel(p)
        y = extract_year_from_path(rel)
        if y is None:
            if strict_year and default_year is None:
                raise ValueError(f"No year segment found in path (strict): {p}")
            y = int(default_year) if default_year is not None else 0
        years.append(y)
        rel_paths.append(rel)
        loc_slug = location_key(rel)
        loc_slugs.append(loc_slug)
        loc_hash = location_hash(loc_slug)
        loc_hashes.append(loc_hash)

        base_embedding = e.astype("float32", copy=False)
        fusion_vec = existing_fusion_lookup.get(p, base_embedding)

        existing_mask_data = existing_mask_lookup.get(p)
        mask_embedding_vec: Optional[np.ndarray] = (
            existing_mask_data.get("mask_embedding") 
            if existing_mask_data and existing_mask_data.get("mask_embedding") is not None 
            else None
        )
        mask_stats: Optional[Dict[str, float]]
        if existing_mask_data:
            raw_stats = existing_mask_data.get("mask_stats")
            match raw_stats:
                case dict():
                    mask_stats = raw_stats
                case _:
                    mask_stats = None
        else:
            mask_stats = None
        mask_path_str: Optional[str] = (
            existing_mask_data.get("mask_path") 
            if existing_mask_data and existing_mask_data.get("mask_path") is not None 
            else None
        )
        mask_image_vec: Optional[np.ndarray] = (
            existing_mask_data.get("mask_image_embedding") if existing_mask_data and existing_mask_data.get("mask_image_embedding") is not None else None
        )
        mask_from_cache = mask_embedding_vec is not None
        mask_image_from_cache = mask_image_vec is not None

        if mask_root is not None:
            need_mask = mask_embedding_vec is None
            candidate = find_mask_path(rel, mask_root) if (need_mask or mask_path_str is None) else (Path(mask_path_str) if mask_path_str else None)
            if need_mask and candidate is not None:
                if embedder is None:
                    embedder = _make_embedder(
                        encoder_name,
                        device=device,
                        batch_size=batch_size,
                        clip_model=clip_model,
                        clip_pretrained=clip_pretrained,
                        eva_model=eva_model,
                        eva_pretrained=eva_pretrained,
                    )
                try:
                    mask_embedding_vec, fusion_candidate, mask_stats = compute_mask_features(
                        Path(p),
                        candidate,
                        embedder,
                        class_defs,
                        base_embedding,
                        focus_weight,
                        mask_color_tolerance,
                    )
                    mask_path_str = str(candidate)
                    fusion_vec = fusion_candidate.astype("float32", copy=False)
                    if mask_embedding_vec is not None:
                        mask_embedding_vec = mask_embedding_vec.astype("float32", copy=False)
                    # mask_image_vec recomputed below
                    mask_image_vec = None
                    mask_from_cache = False
                except Exception as exc:
                    print(f"[warn] mask processing failed for {candidate}: {exc}")
                    mask_embedding_vec = None
                    mask_stats = None

        mask_embeddings_raw.append(mask_embedding_vec)
        fusion_embeddings_list.append(fusion_vec)
        mask_paths.append(mask_path_str)
        mask_stats_list.append(mask_stats)
        if mask_embedding_vec is not None:
            if mask_from_cache:
                reused_mask_vectors += 1
            else:
                new_mask_vectors += 1
        if mask_image_vec is None and mask_path_str and mask_root is not None:
            if embedder is None:
                embedder = _make_embedder(
                    encoder_name,
                    device=device,
                    batch_size=batch_size,
                    clip_model=clip_model,
                    clip_pretrained=clip_pretrained,
                    eva_model=eva_model,
                    eva_pretrained=eva_pretrained,
                )
            try:
                mask_image_vec = embedder.embed_image(mask_path_str).astype("float32", copy=False)
                mask_image_from_cache = False
            except Exception as exc:
                print(f"[warn] mask image embedding failed for {mask_path_str}: {exc}")
        if mask_image_vec is not None:
            if mask_image_from_cache:
                reused_mask_image_vectors += 1
            else:
                new_mask_image_vectors += 1
        mask_image_embeddings_list.append(mask_image_vec)

        if mask_root is not None and idx_loop % 250 == 0:
            print(
                f"...mask pass {idx_loop}/{total_pairs} (reused={reused_mask_vectors}, new={new_mask_vectors})"
            )

        rows_db.append(
            ImageEmbedding(
                image_path=p,
                year=y,
                embedding=base_embedding,
                location_key=loc_slug,
                location_id=loc_hash,
                mask_path=mask_path_str,
                mask_embedding=mask_embedding_vec,
                fusion_embedding=fusion_vec,
                mask_stats=mask_stats,
                mask_image_embedding=mask_image_vec,
            )
        )

    E = np.vstack([vec for _, vec in pairs]).astype("float32", copy=False)
    fusion_matrix = np.vstack(fusion_embeddings_list).astype("float32", copy=False)
    mask_valid = np.array([vec is not None for vec in mask_embeddings_raw], dtype=np.bool_)
    mask_matrix = np.zeros_like(E)
    for idx, vec in enumerate(mask_embeddings_raw):
        if vec is not None:
            mask_matrix[idx] = vec.astype("float32", copy=False)
    mask_image_valid = np.array([vec is not None for vec in mask_image_embeddings_list], dtype=np.bool_)
    mask_image_matrix = np.zeros_like(E)
    for idx, vec in enumerate(mask_image_embeddings_list):
        if vec is not None:
            mask_image_matrix[idx] = vec.astype("float32", copy=False)
    mask_paths_array = np.array(mask_paths, dtype=object)
    mask_stats_serialized = np.array(
        [json.dumps(stats, sort_keys=True) if stats else None for stats in mask_stats_list],
        dtype=object,
    )

    np.save(
        out / "state_loc_keys.npy",
        np.array(loc_slugs, dtype=object),
    )
    np.save(
        out / "state_loc_ids.npy",
        np.array(loc_hashes, dtype=np.int64),
    )
    np.save(
        out / "state_rel_paths.npy",
        np.array(rel_paths, dtype=object)
        )

    meta_tbl = pa.table(
        {
            "path": pa.array(
                paths
                ),
            "year": pa.array(
                years,
                type=pa.int32()
                ),
            "rel_path": pa.array(rel_paths),
            "loc_key": pa.array(loc_slugs, type=pa.string()),
            "loc_id": pa.array(loc_hashes, type=pa.int64()),
            "mask_path": pa.array(mask_paths_array.tolist(), type=pa.string(), from_pandas=True),
            "has_mask": pa.array(mask_valid.tolist(), type=pa.bool_()),
            "has_mask_image": pa.array(mask_image_valid.tolist(), type=pa.bool_()),
            "mask_stats": pa.array(mask_stats_serialized.tolist(), type=pa.string(), from_pandas=True),
            "idx": pa.array(
                list(
                    range(
                        len(
                            paths
                            )
                        )
                    ),
                type=pa.int32()
                ),
            }
        )
    pq.write_table(
        meta_tbl,
        out / "meta.parquet"
        )

    # state embeddings + metadata for reproducibility/debug 
    np.save(out / "state_embeddings.npy", E)
    np.save(out / "state_paths.npy", np.array(paths, dtype=object))
    np.save(out / "state_years.npy", np.array(years, dtype=np.int32))
    np.save(out / "fusion_embeddings.npy", fusion_matrix)
    np.save(out / "mask_embeddings.npy", mask_matrix)
    np.save(out / "mask_valid.npy", mask_valid)
    np.save(out / "mask_paths.npy", mask_paths_array)
    np.save(out / "mask_stats.npy", mask_stats_serialized)
    np.save(out / "mask_image_embeddings.npy", mask_image_matrix)
    np.save(out / "mask_image_valid.npy", mask_image_valid)

    # Optional sharded export (Parquet by default)
    if shard_size and shard_size > 0:
        shard_fmt = shard_format.lower().strip()
        if shard_fmt != "parquet":
            print(f"[warn] shard-format '{shard_format}' not implemented; skipping shard export.")
        else:
            _write_parquet_shards(
                out_dir=out,
                shard_size=int(shard_size),
                paths=paths,
                years=years,
                loc_keys=loc_slugs,
                embeddings=E,
                mask_embeddings=mask_matrix,
                mask_valid=mask_valid,
                fusion_embeddings=fusion_matrix,
                mask_image_embeddings=mask_image_matrix,
                mask_image_valid=mask_image_valid,
                mask_paths=mask_paths_array,
                mask_stats=mask_stats_serialized,
                checkpoint_dir=checkpoint_dir,
                resume=resume_shards,
            )
            _build_faiss_for_shards(
                shard_root=out / "shards",
                spaces=("embedding", "mask_embedding", "fusion_embedding", "mask_image_embedding"),
            )

    status_word = "Extracted" if not reuse_embeddings else "Reused"
    print(f"{status_word} {len(paths)} embeddings (dim={E.shape[1]}). Saved to {out}")
    if mask_root is not None:
        print(
            "Mask feature summary: reused mask vectors={} new mask vectors={} reused mask images={} new mask images={}".format(
                reused_mask_vectors,
                new_mask_vectors,
                reused_mask_image_vectors,
                new_mask_image_vectors,
            )
        )

    #  upsert to pgvector + build FAISS (state and change(delta))
    print("\nStep 4/4: Inserting into pgvector and building FAISS indexes...")
    print("-" * 70)
    with VectorDB(vector_dimension=resolved_vector_dim, **db_cfg) as db:
        upserted = db.insert_embeddings(rows_db)
        print(f"Upserted {upserted} rows into pgvector.")
        if force_reindex:
            print(f"Rebuilding IVFFlat index with lists={ivf_lists}...")
            db.rebuild_ivf_index(ivf_lists=ivf_lists)
        else:
            print("Keeping existing IVFFlat index (use --reindex to rebuild).")
        db.analyze()
        print("ANALYZE complete for image_embeddings.")


    _ = build_faiss_from_array(
        X=E, out_dir=out, base_name="state", dim=E.shape[1],
        use_hnsw=True, m=16, ef_construction=64,
    )

    if mask_valid.any():
        build_faiss_from_array(
            X=mask_matrix[mask_valid],
            out_dir=out,
            base_name="mask",
            dim=mask_matrix.shape[1],
            use_hnsw=True,
            m=16,
            ef_construction=64,
        )
        print(f"Built FAISS mask index with {int(mask_valid.sum())} vectors.")

    if mask_image_valid.any():
        build_faiss_from_array(
            X=mask_image_matrix[mask_image_valid],
            out_dir=out,
            base_name="mask_image",
            dim=mask_image_matrix.shape[1],
            use_hnsw=True,
            m=16,
            ef_construction=64,
        )
        print(f"Built FAISS mask-image index with {int(mask_image_valid.sum())} vectors.")

    delta_base, delta_ids = build_change_vectors(
        loc_slugs, years, E, only_consecutive=delta_consecutive_only
    )
    if len(delta_ids) > 0:
        np.save(out / "delta_ids.npy", np.array(delta_ids, dtype=object))
        np.save(out / "delta_embeddings.npy", delta_base.astype("float32", copy=False))
        _ = build_faiss_from_array(
            X=delta_base,
            out_dir=out,
            base_name="delta",
            dim=delta_base.shape[1],
            use_hnsw=True,
            m=16,
            ef_construction=64,
        )
        print(f"Built FAISS Δ index with {len(delta_ids)} change vectors.")
    else:
        print("No change pairs found (need multiple years per location).")

    delta_fusion, delta_fusion_ids = build_change_vectors(
        loc_slugs, years, fusion_matrix, only_consecutive=delta_consecutive_only
    )
    if len(delta_fusion_ids) > 0:
        np.save(out / "delta_fusion_ids.npy", np.array(delta_fusion_ids, dtype=object))
        np.save(
            out / "delta_fusion_embeddings.npy",
            delta_fusion.astype("float32", copy=False),
        )
        _ = build_faiss_from_array(
            X=delta_fusion,
            out_dir=out,
            base_name="delta_fusion",
            dim=delta_fusion.shape[1],
            use_hnsw=True,
            m=16,
            ef_construction=64,
        )
        print(f"Built FAISS fusion-Δ index with {len(delta_fusion_ids)} change vectors.")

    if mask_valid.any():
        delta_mask, delta_mask_ids = build_change_vectors(
            loc_slugs,
            years,
            mask_matrix,
            only_consecutive=delta_consecutive_only,
            valid_mask=mask_valid,
        )
        if len(delta_mask_ids) > 0:
            np.save(out / "delta_mask_ids.npy", np.array(delta_mask_ids, dtype=object))
            np.save(
                out / "delta_mask_embeddings.npy",
                delta_mask.astype("float32", copy=False),
            )
            _ = build_faiss_from_array(
                X=delta_mask,
                out_dir=out,
                base_name="delta_mask",
                dim=delta_mask.shape[1],
                use_hnsw=True,
                m=16,
                ef_construction=64,
            )
            print(
                f"Built FAISS mask-Δ index with {len(delta_mask_ids)} change vectors."
            )
        else:
            print("Mask-aware Δ vectors skipped (insufficient mask coverage).")

    if mask_image_valid.any():
        delta_mask_image, delta_mask_image_ids = build_change_vectors(
            loc_slugs,
            years,
            mask_image_matrix,
            only_consecutive=delta_consecutive_only,
            valid_mask=mask_image_valid,
        )
        if len(delta_mask_image_ids) > 0:
            np.save(
                out / "delta_mask_image_ids.npy",
                np.array(delta_mask_image_ids, dtype=object),
            )
            np.save(
                out / "delta_mask_image_embeddings.npy",
                delta_mask_image.astype("float32", copy=False),
            )
            _ = build_faiss_from_array(
                X=delta_mask_image,
                out_dir=out,
                base_name="delta_mask_image",
                dim=delta_mask_image.shape[1],
                use_hnsw=True,
                m=16,
                ef_construction=64,
            )
            print(
                f"Built FAISS mask-image Δ index with {len(delta_mask_image_ids)} change vectors."
            )
        else:
            print("Mask-image Δ vectors skipped (insufficient mask tiles).")

    if compute_whitening:
        if whiten_max_samples is not None and E.shape[0] > whiten_max_samples:
            rng = np.random.default_rng(seed=0)
            idx = rng.choice(E.shape[0], int(whiten_max_samples), replace=False)
            sample = E[idx]
            print(f"Computing whitening stats on subsample of {len(sample)} embeddings (cap={whiten_max_samples}).")
        else:
            sample = E
            print(f"Computing whitening stats on all {sample.shape[0]} embeddings.")
        mu, W = compute_whitener(sample)
        np.savez(out / "whiten.npz", mu=mu, W=W)
        print(f"Saved whitening transform to {out / 'whiten.npz'}")
    else:
        print("Skipping whitening statistics (per flag).")

    print("\n" + "=" * 70)
    print("PIPELINE COMPLETE!")
    print("=" * 70)


def main() -> None:
    p = argparse.ArgumentParser(
        description="Embed with open_clip; store in pgvector; build FAISS state/Δ indexes."
    )
    def _normalize_tag(token: str) -> str:
        return "".join(ch if ch.isalnum() else "_" for ch in token).strip("_").lower()

    def _model_tag(encoder: str, clip_model: str, eva_model: str) -> str:
        enc = encoder.strip().lower()
        if enc in {"clip", "clip_vitl14"}:
            return f"{enc}_{_normalize_tag(clip_model if enc == 'clip' else 'ViT-L-14')}"
        if enc == "eva_clip":
            return f"{enc}_{_normalize_tag(eva_model)}"
        return _normalize_tag(enc)

    def _append_suffix(name: str, suffix: str) -> str:
        suffix = f"_{suffix}"
        if name.endswith(suffix):
            return name
        return f"{name}{suffix}"

    def _path_or_none(p: Optional[str]) -> Optional[Path]:
        return Path(p).expanduser() if p else None

    p.add_argument("--folder", required=True, type=str,
                   help="Root of images (expects .../<YEAR>/... in path).")
    p.add_argument("--db-name", default="image_retrieval")
    p.add_argument("--db-user", default="postgres")
    p.add_argument("--db-password", default="postgres")
    p.add_argument("--db-host", default="localhost")
    p.add_argument("--db-port", type=int, default=5432)
    p.add_argument("--encoder", choices=["clip", "eva_clip"], default="clip",
                   help="Embedding backbone to use.")
    p.add_argument("--clip-model", type=str, default="ViT-B-32",
                   help="open_clip model name when --encoder=clip.")
    p.add_argument("--clip-pretrained", type=str, default="laion2b_s34b_b79k",
                   help="open_clip pretrained tag when --encoder=clip.")
    p.add_argument("--eva-model", type=str, default="EVA01-g-14",
                   help="EVA-CLIP model name (open_clip) when --encoder=eva_clip.")
    p.add_argument("--eva-pretrained", type=str, default="laion2b_s4b_b79k",
                   help="EVA-CLIP pretrained tag when --encoder=eva_clip.")
    p.add_argument("--vector-dim", type=int, default=None,
                   help="Embedding dimension override. If omitted, derives from the encoder.")
    p.add_argument("--reset-table-on-dim-mismatch", action="store_true",
                   help="If the DB embedding dimension differs, drop/recreate the embedding tables instead of failing.")
    p.add_argument("--ivf-lists", type=int, default=100)
    p.add_argument("--device", choices=["cpu", "mps", "cuda"], default=None)
    p.add_argument("--batch-size", type=int, default=64)
    p.add_argument("--strict-year", action="store_true",
                   help="Fail if a path lacks a <YEAR> segment.")
    p.add_argument("--default-year", type=int, default=None,
                   help="Fallback year if missing and strict-year is off.")
    p.add_argument("--out-dir", type=str, default="artifacts")
    p.add_argument("--reindex", action="store_true",
                   help="Drop and rebuild the IVFFlat index after inserts.")
    p.add_argument("--skip-whitening", action="store_true",
                   help="Skip computing whitening statistics (mu, W).")
    p.add_argument("--whiten-max-samples", type=int, default=200000,
                   help="Cap on samples used for whitening (0 = use all).")
    p.add_argument("--delta-consecutive-only", action="store_true",
                   help="Limit Δ vectors to consecutive years (default: all combinations).")
    p.add_argument("--mask-dir", type=str, default=None,
                   help="Root directory containing segmentation masks aligned with imagery.")
    p.add_argument(
        "--mask-class",
        action="append",
        dest="mask_classes",
        help=(
            "Mask class mapping in <value>:<label> form (repeat for multiple classes). "
            "You can also pass comma-separated labels in one flag (e.g., 'roadway,sidewalk,crosswalk') "
            "to use the built-in palette, or mix hex/RGB entries like '#ff0000:crosswalk'."
        ),
    )
    p.add_argument("--mask-focus-weight", type=float, default=0.5,
                   help="Maximum blend weight [0-1] for mask-guided fusion embeddings.")
    p.add_argument("--mask-color-tolerance", type=int, default=8,
                   help="Tolerance (0-255) when matching RGB mask colours.")
    p.add_argument("--reuse-embeddings", action="store_true",
                   help="Reuse embeddings from existing artifacts instead of recomputing them.")
    p.add_argument("--shard-size", type=int, default=None,
                   help="If set, write embeddings to sharded artifacts of this many rows (default format: parquet).")
    p.add_argument("--shard-format", type=str, default="parquet",
                   help="Shard format when --shard-size is set. Currently only 'parquet' is implemented.")
    p.add_argument("--checkpoint-dir", type=str, default=None,
                   help="Directory for shard checkpoints/manifest (defaults to <out-dir>/checkpoints).")
    p.add_argument("--resume-shards", action="store_true",
                   help="If manifest exists, skip rewriting shards (useful when resuming).")
    args = p.parse_args()

    db_cfg = {
        "dbname": args.db_name,
        "user": args.db_user,
        "password": args.db_password,
        "host": args.db_host,
        "port": args.db_port,
    }

    mask_class_defs = parse_mask_classes(args.mask_classes) if args.mask_classes else None

    run_pipeline(
        image_folder=args.folder,
        db_cfg=db_cfg,
        vector_dim=args.vector_dim,
        ivf_lists=args.ivf_lists,
        device=args.device,
        batch_size=args.batch_size,
        encoder=args.encoder,
        clip_model=args.clip_model,
        clip_pretrained=args.clip_pretrained,
        eva_model=args.eva_model,
        eva_pretrained=args.eva_pretrained,
        strict_year=bool(args.strict_year),
        default_year=args.default_year,
        out_dir=args.out_dir,
        force_reindex=bool(args.reindex),
        compute_whitening=not bool(args.skip_whitening),
        whiten_max_samples=(None if args.whiten_max_samples <= 0 else int(args.whiten_max_samples)),
        delta_consecutive_only=bool(args.delta_consecutive_only),
        mask_dir=args.mask_dir,
        mask_class_defs=mask_class_defs,
        mask_focus_weight=args.mask_focus_weight,
        mask_color_tolerance=args.mask_color_tolerance,
        reuse_embeddings=bool(args.reuse_embeddings),
        reset_table_on_mismatch=bool(args.reset_table_on_dim_mismatch),
        shard_size=args.shard_size,
        shard_format=args.shard_format,
        checkpoint_dir=_path_or_none(args.checkpoint_dir) or (Path(args.out_dir) / "checkpoints"),
        resume_shards=bool(args.resume_shards),
    )

if __name__ == "__main__":
    main()
